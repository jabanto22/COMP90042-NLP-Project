{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_classifier_vs_ver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1553uHnrWq5ZRfoLwO5W5Q-TAbDCcHJCs",
      "authorship_tag": "ABX9TyPVwe7GyJgdP63vI1WxMpFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jabanto22/NLP-Project/blob/main/project_classifier_vs_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh3QqvrUMccZ",
        "outputId": "eac90bef-cd54-43a3-aa4b-4141f95a7bc6"
      },
      "source": [
        "!pip install torch \n",
        "!pip install torchtext \n",
        "!pip install torchvision \n",
        "!pip install transformers\n",
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grlD1fabFKdx"
      },
      "source": [
        "# Libraries\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# Models\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Training\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Global definitions\n",
        "PROJECT_DATA = '/content/drive/MyDrive/Colab Notebooks/NLP_Project/project-data/'\n",
        "\n",
        "# BERT model\n",
        "BERT_MODEL = \"bert-base-uncased\"\n",
        "\n",
        "# Model Parameters\n",
        "FREEZE = False  # update encoder weights and classification layer weights\n",
        "MAXLEN = 64  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
        "BS = 16  # batch size\n",
        "ITERS_TO_ACCUMULATE = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
        "LR = 3e-5  # learning rate\n",
        "EPOCHS = 4  # number of training epochs\n",
        "\n",
        "# Set seed for reproducibility of results\n",
        "SEED = 1\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Use gpu if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def read_data(filename):\n",
        "    \"\"\"\n",
        "    Read twitter datasets.\n",
        "    \"\"\"\n",
        "    data = pd.DataFrame()\n",
        "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line)\n",
        "            tweet_id = line[0][\"id_str\"]\n",
        "            tweet = p.clean(line[0][\"text\"])\n",
        "            comments = \"\"\n",
        "            for row in line:\n",
        "                # use tweet preprocessor to clean text\n",
        "                comments += \" \" + p.clean(row[\"text\"]) + \".\"\n",
        "            data = data.append({\"id\":tweet_id,\"text\":tweet,\"comments\":comments}, ignore_index=True)\n",
        "    f.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "    \n",
        "def read_label(filename):\n",
        "    \"\"\"\n",
        "    Read class labels.\n",
        "    \"\"\"\n",
        "    label = pd.DataFrame()\n",
        "\n",
        "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "        label = pd.DataFrame.from_dict(json.load(f), orient=\"index\").reset_index()\n",
        "        label.columns = [\"id\", \"label\"]\n",
        "    f.close()\n",
        "\n",
        "    return label\n",
        "\n",
        "    \n",
        "def merge_data_label(data, label):\n",
        "    \"\"\"\n",
        "    Merge train data with class labels and class label codes for prediction.\n",
        "    \"\"\"\n",
        "    data = pd.merge(data, label, on=\"id\", how=\"outer\")\n",
        "    data.label = pd.Categorical(data.label)\n",
        "    class_labels = dict(enumerate(data.label.cat.categories))\n",
        "    data['label'] = data.label.cat.codes\n",
        "\n",
        "    # write predicted labels to json file\n",
        "    with open(PROJECT_DATA + 'labels.json', 'w') as f:\n",
        "        json.dump(class_labels, f, separators=(',', ':'))\n",
        "    f.close()\n",
        "\n",
        "    return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdQrXDxwHS8p"
      },
      "source": [
        "def save_data_to_csv():\n",
        "    \"\"\"\n",
        "    Read and extract datasets from files.\n",
        "    \"\"\"\n",
        "    # read data (jsonl files)\n",
        "    train_data = read_data(PROJECT_DATA + 'train.data.jsonl')\n",
        "    dev_data = read_data(PROJECT_DATA + 'dev.data.jsonl')\n",
        "    test_data = read_data(PROJECT_DATA + 'test.data.jsonl')\n",
        "    covid_data = read_data(PROJECT_DATA + 'covid.data.jsonl')\n",
        "\n",
        "    # read labels (json files)\n",
        "    train_label = read_label(PROJECT_DATA + 'train.label.json')\n",
        "    dev_label = read_label(PROJECT_DATA + 'dev.label.json')\n",
        "\n",
        "    # merge data with class labels\n",
        "    train_data = merge_data_label(train_data, train_label)\n",
        "    dev_data = merge_data_label(dev_data, dev_label)\n",
        "\n",
        "    # write filetered data to csv\n",
        "    open(PROJECT_DATA + 'train.csv','w', newline='').write(train_data.to_csv(index=False))\n",
        "    open(PROJECT_DATA + 'dev.csv','w', newline='').write(dev_data.to_csv(index=False))\n",
        "    open(PROJECT_DATA + 'test.csv','w', newline='').write(test_data.to_csv(index=False))\n",
        "    open(PROJECT_DATA + 'covid.csv','w', newline='').write(covid_data.to_csv(index=False))\n",
        "\n",
        "\n",
        "def check_input_files(filename):\n",
        "    \"\"\"\n",
        "    Check input files if they exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        f = open(filename,'r')\n",
        "        f.close()\n",
        "    except:\n",
        "        # read and process all input datasets\n",
        "        save_data_to_csv()\n",
        "\n",
        "\n",
        "def read_csv_datasets(filename):\n",
        "    # check if input files exist\n",
        "    check_input_files(filename)\n",
        "\n",
        "    # read datasets\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwFLEKsgH3rL"
      },
      "source": [
        "class TweetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='bert-base-uncased'):\n",
        "\n",
        "        self.data = data\n",
        "        self.with_labels = with_labels \n",
        "        \n",
        "        # Initialize BERT tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index, 'text'])\n",
        "        sent2 = str(self.data.loc[index, 'comments'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.maxlen,  \n",
        "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels (train and validation dataset)\n",
        "            label = self.data.loc[index, 'label']\n",
        "            return token_ids, attn_masks, token_type_ids, label  \n",
        "        else:  # for test set that has no labels\n",
        "            return token_ids, attn_masks, token_type_ids"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj7YDMEOH_bP"
      },
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"bert-base-uncased\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        #  Instantiating BERT-based model object\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
        "        \n",
        "        # Freeze bert layers and only train the classification layer weights\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Classification layer\n",
        "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        # output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    @autocast()  # run in mixed precision\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor  containing token ids\n",
        "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
        "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
        "        '''\n",
        "\n",
        "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
        "        output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
        "        \n",
        "        # the last layer hidden-state of the first token of the sequence (classification token) \n",
        "        # further processed by a Linear layer and a Tanh activation function.\n",
        "        logits = self.dropout(output['pooler_output'])\n",
        "        \n",
        "        # Feeding to the classifier layer \n",
        "        logits = self.cls_layer(logits)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lz69dVHJnRe"
      },
      "source": [
        "def plot_train_perf(train_losses, val_losses, train_accuracies, val_accuracies, best_model):\n",
        "    \"\"\"\n",
        "    Create a plot analysis of model loss and accuracy across training epochs.\n",
        "    \"\"\"\n",
        "    acc = train_accuracies\n",
        "    val_acc = val_accuracies\n",
        "    loss = train_losses\n",
        "    val_loss = val_losses\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    path_to_fig = '/content/drive/MyDrive/Colab Notebooks/NLP_Project/models/accuracy-' + best_model + '.png'\n",
        "    fig.savefig(path_to_fig,dpi=300)\n",
        "    fig.show()\n",
        "    plt.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCAehq3sIIqo"
      },
      "source": [
        "def compute_class_weights(train_df):\n",
        "    #compute the class weights\n",
        "    class_weights = compute_class_weight(class_weight='balanced', \n",
        "                                        classes=np.unique(train_df.label.values), \n",
        "                                        y=train_df.label.values)\n",
        "\n",
        "    # converting list of class weights to a tensor\n",
        "    weights = torch.tensor(class_weights[1]/class_weights[0], dtype=torch.float)\n",
        "\n",
        "    # push to GPU\n",
        "    weights = weights.to(device)\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (seq, attn_masks, token_type_ids, labels) in dataloader:\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels).item()\n",
        "            count += 1\n",
        "\n",
        "    return mean_loss / count, mean_acc / count\n",
        "\n",
        "\n",
        "def train(net, bert_model, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
        "\n",
        "    best_loss = np.Inf\n",
        "    best_acc = 0\n",
        "    best_ep = 1\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        iter = 0\n",
        "        for (seq, attn_masks, token_type_ids, labels) in train_loader:\n",
        "            iter += 1\n",
        "            #Converting these to tensors\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            # Enables autocasting for the forward pass (model + loss)\n",
        "            with autocast():\n",
        "                # Obtaining the logits from the model\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "                # Computing loss\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
        "\n",
        "                # Computing accuracy\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                total_acc += acc\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if iter % iters_to_accumulate == 0:\n",
        "                # Optimization step\n",
        "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
        "                # otherwise, opti.step() is skipped.\n",
        "                scaler.step(opti)\n",
        "                # Updates the scale for next iteration.\n",
        "                scaler.update()\n",
        "                # Adjust the learning rate based on the number of iterations.\n",
        "                lr_scheduler.step()\n",
        "                # Clear gradients\n",
        "                opti.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if iter % print_every == 0 and iter != 0:  # Print training loss information\n",
        "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
        "                      .format(iter, nb_iterations, ep+1, running_loss / print_every))\n",
        "                \n",
        "                total_loss += running_loss\n",
        "                \n",
        "                running_loss = 0.0\n",
        "\n",
        "        train_losses.append(total_loss / len(train_loader))\n",
        "        train_accuracies.append(total_acc / len(train_loader))\n",
        "        \n",
        "        val_loss, val_acc = evaluate(net, device, criterion, val_loader)  # Compute validation loss and accuracy\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        print(\"\\nEpoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(\"Best validation loss improved from {} to {}\\n\".format(best_loss, val_loss))\n",
        "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "            best_loss = val_loss\n",
        "            best_acc = val_acc\n",
        "            best_ep = ep + 1\n",
        "\n",
        "    # Saving the model\n",
        "    path_to_model = '/content/drive/MyDrive/Colab Notebooks/NLP_Project/models/{}_lr_{}_val_loss_{}_acc_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), round(best_acc, 5), best_ep)\n",
        "    best_model = '{}_lr_{}_val_loss_{}_acc_{}_ep_{}'.format(bert_model, lr, round(best_loss, 5), round(best_acc, 5), best_ep)\n",
        "    torch.save(net_copy.state_dict(), path_to_model)\n",
        "    print(\"Finished training!\")\n",
        "    print(\"The model has been saved in {}\".format(path_to_model))\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Plot performance of model on each epoch\n",
        "    plot_train_perf(train_losses, val_losses, train_accuracies, val_accuracies, best_model)\n",
        "\n",
        "    return path_to_model\n",
        "\n",
        "\n",
        "def finetuneBERT(train_df, dev_df):\n",
        "    destination_path = '/content/drive/MyDrive/Colab Notebooks/NLP_Project/models'\n",
        "    try:\n",
        "        os.makedirs(destination_path)\n",
        "        print(\"Directory:\", destination_path, \"created.\")\n",
        "    except:\n",
        "        print(\"Directory:\", destination_path, \"already exists.\")\n",
        "\n",
        "    # training parameters\n",
        "    freeze_bert = FREEZE\n",
        "    maxlen = MAXLEN\n",
        "    bs = BS\n",
        "    iters_to_accumulate = ITERS_TO_ACCUMULATE\n",
        "    lr = LR\n",
        "    epochs = EPOCHS\n",
        "\n",
        "    # Read train and validation datasets\n",
        "    print(\"Reading training data...\")\n",
        "    train_set = TweetDataset(train_df, maxlen, BERT_MODEL)\n",
        "    print(\"Reading validation data...\")\n",
        "    val_set = TweetDataset(dev_df, maxlen, BERT_MODEL)\n",
        "    \n",
        "    # Create instances of training and validation dataloaders\n",
        "    train_loader = DataLoader(train_set, batch_size=bs, num_workers=2)\n",
        "    val_loader = DataLoader(val_set, batch_size=bs, num_workers=2)\n",
        "    print(\"Done preprocessing training and development data.\")\n",
        "\n",
        "    net = SentencePairClassifier(BERT_MODEL, freeze_bert=freeze_bert)\n",
        "    net.to(device)\n",
        "\n",
        "    # model parameters for fine-tuning\n",
        "    weights = compute_class_weights(train_df)\n",
        "    criterion = nn.BCEWithLogitsLoss(weight=weights)\n",
        "    opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "    num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "    num_training_steps = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "    lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "    # start training for downstream task\n",
        "    path_to_model = train(net, BERT_MODEL, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)\n",
        "\n",
        "    return path_to_model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4DuZzMvKC-U"
      },
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def predict_to_file(net, device, dataloader, with_labels=True, result_file=PROJECT_DATA + \"output.txt\"):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if with_labels:\n",
        "            for seq, attn_masks, token_type_ids, _ in dataloader:\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "        else:\n",
        "            for seq, attn_masks, token_type_ids in dataloader:\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    w.close()\n",
        "\n",
        "\n",
        "def extract_class_labels():\n",
        "    # read class labels from json file\n",
        "    label = pd.DataFrame()\n",
        "    with open(PROJECT_DATA + 'labels.json', 'r', encoding=\"utf8\") as f:\n",
        "        label = json.load(f)\n",
        "    f.close()\n",
        "    return label\n",
        "\n",
        "\n",
        "def save_result(data, path_to_output_file=PROJECT_DATA + \"output.txt\", save_file=PROJECT_DATA + \"output.json\"):\n",
        "    \"\"\"\n",
        "    Save predictions on the test data to json file.\n",
        "    \"\"\"\n",
        "    probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # read prediction probabilities from file\n",
        "    preds_test=(probs_test>=0.5).astype('uint8') # predicted labels using the fixed threshold of 0.5\n",
        "\n",
        "    labels = extract_class_labels()\n",
        "    pred_label = {}\n",
        "    for i in range(len(preds_test)):\n",
        "        code = str(preds_test[i])\n",
        "        text_id = str(data.iloc[i]['id'])\n",
        "        pred_label[text_id] = labels[code]\n",
        "        \n",
        "    # write predicted labels to json file\n",
        "    with open(PROJECT_DATA + 'test-output.json', 'w') as f:\n",
        "        json.dump(pred_label, f, separators=(',', ':'))\n",
        "    f.close()\n",
        "\n",
        "    print(\"Predictions are available in : {}\".format(save_file))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoDv3VDbPDw1",
        "outputId": "9bd9fbf7-1bda-419a-82e4-c854987a5746"
      },
      "source": [
        "# read train and dev datasets\n",
        "train_df = read_csv_datasets(PROJECT_DATA + 'train.csv')\n",
        "dev_df = read_csv_datasets(PROJECT_DATA + 'dev.csv')\n",
        "\n",
        "path_to_model = finetuneBERT(train_df, dev_df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory: /content/drive/MyDrive/Colab Notebooks/NLP_Project/models already exists.\n",
            "Reading training data...\n",
            "Reading validation data...\n",
            "Done preprocessing training and development data.\n",
            "Iteration 58/291 of epoch 1 complete. Loss : 0.5676862176122337 \n",
            "Iteration 116/291 of epoch 1 complete. Loss : 0.4282150173495556 \n",
            "Iteration 174/291 of epoch 1 complete. Loss : 0.38165400588306886 \n",
            "Iteration 232/291 of epoch 1 complete. Loss : 0.3509611385906565 \n",
            "Iteration 290/291 of epoch 1 complete. Loss : 0.3654688457990515 \n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.6043922401763298\n",
            "Best validation loss improved from inf to 0.6043922401763298\n",
            "\n",
            "Iteration 58/291 of epoch 2 complete. Loss : 0.3304579137214299 \n",
            "Iteration 116/291 of epoch 2 complete. Loss : 0.24677554607905192 \n",
            "Iteration 174/291 of epoch 2 complete. Loss : 0.23875260301705065 \n",
            "Iteration 232/291 of epoch 2 complete. Loss : 0.24365975887610994 \n",
            "Iteration 290/291 of epoch 2 complete. Loss : 0.218277288857719 \n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.5398962449383091\n",
            "Best validation loss improved from 0.6043922401763298 to 0.5398962449383091\n",
            "\n",
            "Iteration 58/291 of epoch 3 complete. Loss : 0.20075176559902472 \n",
            "Iteration 116/291 of epoch 3 complete. Loss : 0.14982255818001156 \n",
            "Iteration 174/291 of epoch 3 complete. Loss : 0.1217803783586313 \n",
            "Iteration 232/291 of epoch 3 complete. Loss : 0.1652117142582248 \n",
            "Iteration 290/291 of epoch 3 complete. Loss : 0.12390266956183417 \n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.5901680898827475\n",
            "Iteration 58/291 of epoch 4 complete. Loss : 0.10931640641828036 \n",
            "Iteration 116/291 of epoch 4 complete. Loss : 0.09023590201255062 \n",
            "Iteration 174/291 of epoch 4 complete. Loss : 0.06938638427326906 \n",
            "Iteration 232/291 of epoch 4 complete. Loss : 0.07927195714176472 \n",
            "Iteration 290/291 of epoch 4 complete. Loss : 0.06879784530510419 \n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.6269492193855144\n",
            "Finished training!\n",
            "The model has been saved in /content/drive/MyDrive/Colab Notebooks/NLP_Project/models/bert-base-uncased_lr_3e-05_val_loss_0.5399_acc_0.88514_ep_2.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvH0aHBjXNA",
        "outputId": "1e2ca108-12d5-4028-b706-80a5bf2b23c0"
      },
      "source": [
        "def test_prediction(path_to_model=\"/content/drive/MyDrive/Colab Notebooks/NLP_Project/models/bert-base-uncased_lr_3e-05_val_loss_0.5399_acc_0.88514_ep_2.pt\"):\n",
        "\n",
        "    # load the best model for classification task\n",
        "    model = SentencePairClassifier(BERT_MODEL)\n",
        "    print(\"\\nLoading the weights of the model...\")\n",
        "    model.load_state_dict(torch.load(path_to_model, map_location=device))\n",
        "\n",
        "    # use the trained model to predict class labels for the test set\n",
        "    print(\"Reading test data...\")\n",
        "    test_df = read_csv_datasets(PROJECT_DATA + 'test.csv')\n",
        "    test_set = TweetDataset(test_df, MAXLEN, False, BERT_MODEL)\n",
        "    test_loader = DataLoader(test_set, batch_size=BS, num_workers=2)\n",
        "    print(\"Done preprocessing test data.\")\n",
        "\n",
        "    print(\"Predicting on test data...\")\n",
        "    path_to_output_file = PROJECT_DATA + 'test-output-probabilities.txt'\n",
        "    predict_to_file(net=model, device=device, dataloader=test_loader, with_labels=False,\n",
        "                    result_file=path_to_output_file)\n",
        "    print(\"\\nTest classification probabilities are available in : {}\".format(path_to_output_file))\n",
        "\n",
        "    path_to_output_json = PROJECT_DATA + 'test-output.json'\n",
        "    save_result(test_df, path_to_output_file, path_to_output_json)\n",
        "\n",
        "test_prediction()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading the weights of the model...\n",
            "Reading test data...\n",
            "Done preprocessing test data.\n",
            "Predicting on test data...\n",
            "\n",
            "Test classification probabilities are available in : /content/drive/MyDrive/Colab Notebooks/NLP_Project/project-data/test-output-probabilities.txt\n",
            "Predictions are available in : /content/drive/MyDrive/Colab Notebooks/NLP_Project/project-data/test-output.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hU4LMiRlgv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff34fe0-a996-4a1f-9ade-a026956ad954"
      },
      "source": [
        "def covid_prediction(path_to_model=\"/content/drive/MyDrive/Colab Notebooks/NLP_Project/models/bert-base-uncased_lr_3e-05_val_loss_0.5399_acc_0.88514_ep_2.pt\"):\n",
        "\n",
        "    # load the best model for classification task\n",
        "    model = SentencePairClassifier(BERT_MODEL)\n",
        "    print(\"\\nLoading the weights of the model...\")\n",
        "    model.load_state_dict(torch.load(path_to_model, map_location=device))\n",
        "\n",
        "    # use the trained model to predict class labels for the test set\n",
        "    print(\"Reading test data...\")\n",
        "    covid_df = read_csv_datasets(PROJECT_DATA + 'covid.csv')\n",
        "    covid_set = TweetDataset(covid_df, MAXLEN, False, BERT_MODEL)\n",
        "    covid_loader = DataLoader(covid_set, batch_size=BS, num_workers=2)\n",
        "    print(\"Done preprocessing covid data.\")\n",
        "\n",
        "    print(\"Predicting on covid data...\")\n",
        "    path_to_output_file = PROJECT_DATA + 'covid-output-probabilities.txt'\n",
        "    predict_to_file(net=model, device=device, dataloader=covid_loader, with_labels=False,\n",
        "                    result_file=path_to_output_file)\n",
        "    print(\"\\nTest classification probabilities are available in : {}\".format(path_to_output_file))\n",
        "\n",
        "    path_to_output_json = PROJECT_DATA + 'covid-output.json'\n",
        "    save_result(covid_df, path_to_output_file, path_to_output_json)\n",
        "\n",
        "covid_prediction()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading the weights of the model...\n",
            "Reading test data...\n",
            "Done preprocessing covid data.\n",
            "Predicting on covid data...\n",
            "\n",
            "Test classification probabilities are available in : /content/drive/MyDrive/Colab Notebooks/NLP_Project/project-data/covid-output-probabilities.txt\n",
            "Predictions are available in : /content/drive/MyDrive/Colab Notebooks/NLP_Project/project-data/covid-output.json\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}