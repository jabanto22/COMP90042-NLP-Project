{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh3QqvrUMccZ",
    "outputId": "cf42878a-7c9e-4f21-bed1-bd9fce0ed6a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch) (1.19.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: torchtext in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchtext) (1.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchtext) (4.59.0)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchtext) (1.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchtext) (2.24.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch==1.8.1->torchtext) (3.7.4.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch==1.8.1->torchtext) (0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->torchtext) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->torchtext) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torchvision) (1.19.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch==1.8.1->torchvision) (0.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (4.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (1.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: tweet-preprocessor in c:\\users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch \n",
    "!pip install torchtext \n",
    "!pip install torchvision \n",
    "!pip install transformers\n",
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYHbZ-1Er7Ec"
   },
   "source": [
    "# Rumour Classification\n",
    "A BERT based binary classification task applied on main tweets and replies to predict whether the main tweet is a rumour or not. The pre-trained BERT model is implemented and fine-tuned using PyTorch and Huggingface transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YKb5s5LupdL"
   },
   "source": [
    "## Using PyTorch and Huggingface libraries\n",
    "The following are the libraries needed to preprocess the Twitter files and fine-tune the pre-trained BERT model for our Classification task. It is important to notice that we set all seeds to have reproducible results when we re-run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "grlD1fabFKdx"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Training\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global definitions\n",
    "PROJECT_DATA = './project-data/'\n",
    "MODELS = '/models'\n",
    "\n",
    "# BERT model\n",
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "\n",
    "# Model Parameters\n",
    "FREEZE = False  # update encoder weights and classification layer weights\n",
    "MAXLEN = 64  # maximum length of the tokenized input sentence pair\n",
    "BS = 16  # batch size\n",
    "ITERS_TO_ACCUMULATE = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
    "LR = 3e-5  # learning rate\n",
    "EPOCHS = 4  # number of training epochs\n",
    "\n",
    "# Set seed for reproducibility of results\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M62vHtytM_3"
   },
   "source": [
    "## Reading and Merging data with class labels\n",
    "We define below the methods for reading and merging twitter data from JSONL files with class labels from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wIaPJhYATrvZ"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read twitter datasets.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            tweet_id = line[0][\"id_str\"]\n",
    "            tweet = p.clean(line[0][\"text\"])\n",
    "            comments = \"\"\n",
    "            for row in line:\n",
    "                # use tweet preprocessor to clean text (remove URLs, mentions, reserved words, emojis, and smileys)\n",
    "                comments += \" \" + p.clean(row[\"text\"]) + \".\"\n",
    "            data = data.append({\"id\":tweet_id,\"text\":tweet,\"comments\":comments}, ignore_index=True)\n",
    "    f.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "def read_label(filename):\n",
    "    \"\"\"\n",
    "    Read class labels.\n",
    "    \"\"\"\n",
    "    label = pd.DataFrame()\n",
    "\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        label = pd.DataFrame.from_dict(json.load(f), orient=\"index\").reset_index()\n",
    "        label.columns = [\"id\", \"label\"]\n",
    "    f.close()\n",
    "\n",
    "    return label\n",
    "\n",
    "    \n",
    "def merge_data_label(data, label):\n",
    "    \"\"\"\n",
    "    Merge train data with class labels and class label codes for prediction.\n",
    "    \"\"\"\n",
    "    data = pd.merge(data, label, on=\"id\", how=\"outer\")\n",
    "    data.label = pd.Categorical(data.label)\n",
    "    class_labels = dict(enumerate(data.label.cat.categories))\n",
    "    data['label'] = data.label.cat.codes\n",
    "\n",
    "    # write predicted labels to json file\n",
    "    with open(PROJECT_DATA + 'labels.json', 'w') as f:\n",
    "        json.dump(class_labels, f, separators=(',', ':'))\n",
    "    f.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zb1fmOetvr9"
   },
   "source": [
    "## Generating CSV data files\n",
    "The following methods will be used to save the loaded and merged train, validation, and test Twitter files to CSV files for later use. The CSV files are generated so as not to repeatedly load the large JSONL files to memory when re-running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TdQrXDxwHS8p"
   },
   "outputs": [],
   "source": [
    "def save_data_to_csv():\n",
    "    \"\"\"\n",
    "    Read and extract datasets from files.\n",
    "    \"\"\"\n",
    "    # read data (jsonl files)\n",
    "    train_data = read_data(PROJECT_DATA + 'train.data.jsonl')\n",
    "    dev_data = read_data(PROJECT_DATA + 'dev.data.jsonl')\n",
    "    test_data = read_data(PROJECT_DATA + 'test.data.jsonl')\n",
    "    covid_data = read_data(PROJECT_DATA + 'covid.data.jsonl')\n",
    "\n",
    "    # read labels (json files)\n",
    "    train_label = read_label(PROJECT_DATA + 'train.label.json')\n",
    "    dev_label = read_label(PROJECT_DATA + 'dev.label.json')\n",
    "\n",
    "    # merge data with class labels\n",
    "    train_data = merge_data_label(train_data, train_label)\n",
    "    dev_data = merge_data_label(dev_data, dev_label)\n",
    "\n",
    "    # write filetered data to csv\n",
    "    open(PROJECT_DATA + 'train.csv','w', newline='').write(train_data.to_csv(index=False))\n",
    "    open(PROJECT_DATA + 'dev.csv','w', newline='').write(dev_data.to_csv(index=False))\n",
    "    open(PROJECT_DATA + 'test.csv','w', newline='').write(test_data.to_csv(index=False))\n",
    "    open(PROJECT_DATA + 'covid.csv','w', newline='').write(covid_data.to_csv(index=False))\n",
    "\n",
    "\n",
    "def check_input_files(filename):\n",
    "    \"\"\"\n",
    "    Check input files if they exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = open(filename,'r')\n",
    "        f.close()\n",
    "    except:\n",
    "        # read and process all input datasets\n",
    "        save_data_to_csv()\n",
    "\n",
    "\n",
    "def read_csv_datasets(filename):\n",
    "    # check if input files exist\n",
    "    check_input_files(filename)\n",
    "\n",
    "    # read datasets\n",
    "    df = pd.read_csv(filename, dtype={'id':str})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFSFaXqxyhaH"
   },
   "source": [
    "## Preparing and Tokenizing our Dataset\n",
    "We define below a custom Dataset class to tokenize a pair of tweets (main tweet, main tweet+replies) using pre-trained BERT tokenizer to get token ids, attention masks, and token type ids converted to tensors which will be used as inputs to BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KwFLEKsgH3rL"
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='bert-base-uncased'):\n",
    "\n",
    "        self.data = data\n",
    "        self.with_labels = with_labels \n",
    "        \n",
    "        # Initialize BERT tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
    "        sent1 = str(self.data.loc[index, 'text'])\n",
    "        sent2 = str(self.data.loc[index, 'comments'])\n",
    "\n",
    "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
    "        encoded_pair = self.tokenizer(sent1, sent2, \n",
    "                                      padding='max_length',  # Pad to max_length\n",
    "                                      truncation=True,  # Truncate to max_length\n",
    "                                      max_length=self.maxlen,  \n",
    "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
    "        \n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
    "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
    "\n",
    "        if self.with_labels:  # True if the dataset has labels (train and validation dataset)\n",
    "            label = self.data.loc[index, 'label']\n",
    "            return token_ids, attn_masks, token_type_ids, label  \n",
    "        else:  # for test set that has no labels\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-3LFa1m0LD5"
   },
   "source": [
    "## BERT model\n",
    "This is the BERT model, a simple feed forward network with one dropout layer before feeding the results to the classifier layer. We will not be feezing the BERT layers for this task as we want to train all layers for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sj7YDMEOH_bP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", freeze_bert=False):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        #  Instantiating BERT-based model object\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
    "        \n",
    "        # Freeze bert layers and only train the classification layer weights\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classification layer\n",
    "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        # output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()  # run in mixed precision\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
    "        output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "        \n",
    "        # the last layer hidden-state of the first token of the sequence (classification token) \n",
    "        # further processed by a Linear layer and a Tanh activation function.\n",
    "        logits = self.dropout(output['pooler_output'])\n",
    "        \n",
    "        # Feeding to the classifier layer \n",
    "        logits = self.cls_layer(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMQ11dOzBaFr"
   },
   "source": [
    "### Plotting accuracy and loss to file\n",
    "The following method will be used to plot the training and validation accuracy and loss of the model on each epoch while fine-tuning the BERT model for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5lz69dVHJnRe"
   },
   "outputs": [],
   "source": [
    "def plot_train_perf(train_losses, val_losses, train_accuracies, val_accuracies, best_model):\n",
    "    \"\"\"\n",
    "    Create a plot analysis of model loss and accuracy across training epochs.\n",
    "    \"\"\"\n",
    "    acc = train_accuracies\n",
    "    val_acc = val_accuracies\n",
    "    loss = train_losses\n",
    "    val_loss = val_losses\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    path_to_fig = MODELS + '/accuracy-' + best_model + '.png'\n",
    "    fig.savefig(path_to_fig,dpi=300)\n",
    "    fig.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBN7RnUnB5TJ"
   },
   "source": [
    "### Fine-tuning the BERT model\n",
    "The following methods will be used for fine-tuning the pre-trained BERT model with **AdamW** optimizer and using **BCEWithLogitsLoss** loss function. The class weights are computed and feed as input to the loss function to handle any imbalance in the training data. Fine-tuning takes 4 epochs, but we will notice that this is more than enough to train the BERT model for our task as loss starts to increase from the 3rd epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wCAehq3sIIqo"
   },
   "outputs": [],
   "source": [
    "def compute_class_weights(train_df):\n",
    "    #compute the class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                        classes=np.unique(train_df.label.values), \n",
    "                                        y=train_df.label.values)\n",
    "\n",
    "    # converting list of class weights to a tensor\n",
    "    weights = torch.tensor(class_weights[1]/class_weights[0], dtype=torch.float)\n",
    "\n",
    "    # push to GPU\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def evaluate(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (seq, attn_masks, token_type_ids, labels) in dataloader:\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels).item()\n",
    "            count += 1\n",
    "\n",
    "    return mean_loss / count, mean_acc / count\n",
    "\n",
    "\n",
    "def train(net, bert_model, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_acc = 0\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        iter = 0\n",
    "        for (seq, attn_masks, token_type_ids, labels) in train_loader:\n",
    "            iter += 1\n",
    "            #Converting these to tensors\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "    \n",
    "            # Enables autocasting for the forward pass (model + loss)\n",
    "            with autocast():\n",
    "                # Obtaining the logits from the model\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "\n",
    "                # Computing loss\n",
    "                loss = criterion(logits.squeeze(-1), labels.float())\n",
    "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "\n",
    "                # Computing accuracy\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                total_acc += acc\n",
    "\n",
    "            # Backpropagating the gradients\n",
    "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if iter % iters_to_accumulate == 0:\n",
    "                # Optimization step\n",
    "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
    "                # otherwise, opti.step() is skipped.\n",
    "                scaler.step(opti)\n",
    "                # Updates the scale for next iteration.\n",
    "                scaler.update()\n",
    "                # Adjust the learning rate based on the number of iterations.\n",
    "                lr_scheduler.step()\n",
    "                # Clear gradients\n",
    "                opti.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if iter % print_every == 0 and iter != 0:  # Print training loss information\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(iter, nb_iterations, ep+1, running_loss / print_every))\n",
    "                \n",
    "                total_loss += running_loss\n",
    "                \n",
    "                running_loss = 0.0\n",
    "\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_accuracies.append(total_acc / len(train_loader))\n",
    "        \n",
    "        val_loss, val_acc = evaluate(net, device, criterion, val_loader)  # Compute validation loss and accuracy\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(\"\\nEpoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\\n\".format(best_loss, val_loss))\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_acc = val_acc\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model = MODELS + '/{}_lr_{}_val_loss_{}_acc_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), round(best_acc, 5), best_ep)\n",
    "    best_model = '{}_lr_{}_val_loss_{}_acc_{}_ep_{}'.format(bert_model, lr, round(best_loss, 5), round(best_acc, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"Finished training!\")\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Plot performance of model on each epoch\n",
    "    plot_train_perf(train_losses, val_losses, train_accuracies, val_accuracies, best_model)\n",
    "\n",
    "    return path_to_model\n",
    "\n",
    "\n",
    "def finetuneBERT(train_df, dev_df):\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(MODELS)\n",
    "        print(\"Directory:\", MODELS, \"created.\")\n",
    "    except:\n",
    "        print(\"Directory:\", MODELS, \"already exists.\")\n",
    "\n",
    "    # Read train and validation datasets\n",
    "    print(\"Reading training data...\")\n",
    "    train_set = TweetDataset(train_df, MAXLEN, BERT_MODEL)\n",
    "    print(\"Reading validation data...\")\n",
    "    val_set = TweetDataset(dev_df, MAXLEN, BERT_MODEL)\n",
    "    \n",
    "    # Create instances of training and validation dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=BS, num_workers=2)\n",
    "    val_loader = DataLoader(val_set, batch_size=BS, num_workers=2)\n",
    "    print(\"Done preprocessing training and development data.\")\n",
    "\n",
    "    net = SentencePairClassifier(BERT_MODEL, freeze_bert=FREEZE)\n",
    "    net.to(device)\n",
    "\n",
    "    # model parameters for fine-tuning\n",
    "    weights = compute_class_weights(train_df)\n",
    "    criterion = nn.BCEWithLogitsLoss(weight=weights)\n",
    "    opti = AdamW(net.parameters(), lr=LR, weight_decay=1e-2)\n",
    "    num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "    num_training_steps = (len(train_loader) // ITERS_TO_ACCUMULATE) * EPOCHS  # Necessary to take into account Gradient accumulation\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    # start training for downstream task\n",
    "    path_to_model = train(net, BERT_MODEL, criterion, opti, LR, lr_scheduler, train_loader, val_loader, EPOCHS, ITERS_TO_ACCUMULATE)\n",
    "\n",
    "    return path_to_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKHj2bzQDRgw"
   },
   "source": [
    "### Making predictions using fine-tuned BERT model\n",
    "The following methods will be used to make predictions using the BERT model we trained in the previous steps. We will also be saving the predictions to a file for potential future use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E4DuZzMvKC-U"
   },
   "outputs": [],
   "source": [
    "def get_probs_from_logits(logits):\n",
    "    \"\"\"\n",
    "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def predict_to_file(net, device, dataloader, with_labels=True, result_file=PROJECT_DATA + \"output.txt\"):\n",
    "    \"\"\"\n",
    "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    w = open(result_file, 'w')\n",
    "    probs_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if with_labels:\n",
    "            for seq, attn_masks, token_type_ids, _ in dataloader:\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "        else:\n",
    "            for seq, attn_masks, token_type_ids in dataloader:\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "\n",
    "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
    "    w.close()\n",
    "\n",
    "\n",
    "def extract_class_labels():\n",
    "    # read class labels from json file\n",
    "    label = pd.DataFrame()\n",
    "    with open(PROJECT_DATA + 'labels.json', 'r', encoding=\"utf8\") as f:\n",
    "        label = json.load(f)\n",
    "    f.close()\n",
    "    return label\n",
    "\n",
    "\n",
    "def save_result(data, path_to_output_file=PROJECT_DATA + \"output.txt\", save_file=PROJECT_DATA + \"output.json\"):\n",
    "    \"\"\"\n",
    "    Save predictions on the test data to json file.\n",
    "    \"\"\"\n",
    "    probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # read prediction probabilities from file\n",
    "    preds_test=(probs_test>=0.5).astype('uint8') # predicted labels using the fixed threshold of 0.5\n",
    "\n",
    "    labels = extract_class_labels()\n",
    "    pred_label = {}\n",
    "    for i in range(len(preds_test)):\n",
    "        code = str(preds_test[i])\n",
    "        text_id = str(data.iloc[i]['id'])\n",
    "        pred_label[text_id] = labels[code]\n",
    "        \n",
    "    # write predicted labels to json file\n",
    "    with open(save_file, 'w') as f:\n",
    "        json.dump(pred_label, f, separators=(',', ':'))\n",
    "    f.close()\n",
    "\n",
    "    print(\"Predictions are available in : {}\".format(save_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QiOuNxsEpZU"
   },
   "source": [
    "## Training the model\n",
    "Finally, we fine-tune and train BERT model for our classification task using training and validation sets from Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoDv3VDbPDw1",
    "outputId": "8883c42a-6d4c-483d-f83a-20e69d48866d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /models already exists.\n",
      "Reading training data...\n",
      "Reading validation data...\n",
      "Done preprocessing training and development data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jean_\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ee98bb2ee2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdev_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPROJECT_DATA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'dev.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpath_to_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetuneBERT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-58661bfa5c5e>\u001b[0m in \u001b[0;36mfinetuneBERT\u001b[1;34m(train_df, dev_df)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;31m# start training for downstream task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mpath_to_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBERT_MODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mITERS_TO_ACCUMULATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath_to_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-58661bfa5c5e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, bert_model, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0miter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0miter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;31m#Converting these to tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    912\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "# read train and dev datasets\n",
    "train_df = read_csv_datasets(PROJECT_DATA + 'train.csv')\n",
    "dev_df = read_csv_datasets(PROJECT_DATA + 'dev.csv')\n",
    "\n",
    "path_to_model = finetuneBERT(train_df, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG9nObyHFlBF"
   },
   "source": [
    "## Predicting on Test Data\n",
    "Using the best model we saved after fine-tuning the pre-trained BERT model, we now predict the classification of the tweets on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnvH0aHBjXNA",
    "outputId": "4f8ac3dd-f294-49a5-cff7-037dfa651fbd"
   },
   "outputs": [],
   "source": [
    "def test_prediction():\n",
    "\n",
    "    # path_to_model = MODELS + \"/bert-base-uncased_lr_3e-05_val_loss_0.5399_acc_0.88514_ep_2.pt\"\n",
    "\n",
    "    # load the best model for classification task\n",
    "    model = SentencePairClassifier(BERT_MODEL)\n",
    "    print(\"\\nLoading the weights of the model...\")\n",
    "    model.load_state_dict(torch.load(path_to_model, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # use the trained model to predict class labels for the test set\n",
    "    print(\"Reading test data...\")\n",
    "    test_df = read_csv_datasets(PROJECT_DATA + 'test.csv')\n",
    "    test_set = TweetDataset(test_df, MAXLEN, False, BERT_MODEL)\n",
    "    test_loader = DataLoader(test_set, batch_size=BS, num_workers=2)\n",
    "    print(\"Done preprocessing test data.\")\n",
    "\n",
    "    print(\"Predicting on test data...\")\n",
    "    path_to_output_file = PROJECT_DATA + 'test-output-probabilities.txt'\n",
    "    predict_to_file(net=model, device=device, dataloader=test_loader, with_labels=False,\n",
    "                    result_file=path_to_output_file)\n",
    "    print(\"\\nTest classification probabilities are available in : {}\".format(path_to_output_file))\n",
    "\n",
    "    path_to_output_json = PROJECT_DATA + 'test-output.json'\n",
    "    save_result(test_df, path_to_output_file, path_to_output_json)\n",
    "\n",
    "test_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcvD9TAUrT-0"
   },
   "source": [
    "# Rumour Analysis\n",
    "A simple sentiment analysis using TextBlob on and replies to main tweets classified as rumours and non-rumours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOflftUTvImy"
   },
   "source": [
    "## Predicting class labels of Covid data\n",
    "Before we can start doing some analysis on the Covid dataset, we first predict the classification of the tweets as either a rumour or non-rumour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hU4LMiRlgv_"
   },
   "outputs": [],
   "source": [
    "def covid_prediction():\n",
    "\n",
    "    # path_to_model = MODELS + \"/bert-base-uncased_lr_3e-05_val_loss_0.5399_acc_0.88514_ep_2.pt\"\n",
    "\n",
    "    # load the best model for classification task\n",
    "    model = SentencePairClassifier(BERT_MODEL)\n",
    "    print(\"\\nLoading the weights of the model...\")\n",
    "    model.load_state_dict(torch.load(path_to_model, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # use the trained model to predict class labels for the test set\n",
    "    print(\"Reading covid data...\")\n",
    "    covid_df = read_csv_datasets(PROJECT_DATA + 'covid.csv')\n",
    "    covid_set = TweetDataset(covid_df, MAXLEN, False, BERT_MODEL)\n",
    "    covid_loader = DataLoader(covid_set, batch_size=BS, num_workers=2)\n",
    "    print(\"Done preprocessing covid data.\")\n",
    "\n",
    "    print(\"Predicting on covid data...\")\n",
    "    path_to_output_file = PROJECT_DATA + 'covid-output-probabilities.txt'\n",
    "    predict_to_file(net=model, device=device, dataloader=covid_loader, with_labels=False,\n",
    "                    result_file=path_to_output_file)\n",
    "    print(\"\\nCovid classification probabilities are available in : {}\".format(path_to_output_file))\n",
    "\n",
    "    path_to_output_json = PROJECT_DATA + 'covid-output.json'\n",
    "    save_result(covid_df, path_to_output_file, path_to_output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfQtcWFOvsE8"
   },
   "source": [
    "## Preparing the Covid dataset\n",
    "Next, we prepare the Covid dataset containing the replies to and the main tweets which will be our inputs for the Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UY_VNw9pTTK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_covid_data():\n",
    "    \"\"\"\n",
    "    Read covid datasets.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    with open(PROJECT_DATA + 'covid.data.jsonl', 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            tweet_id = line[0][\"id_str\"]\n",
    "            datetime = line[0][\"created_at\"]\n",
    "            hashtags = re.findall(r\"#(\\w+)\", line[0][\"text\"])\n",
    "\n",
    "            tweet = p.clean(line[0][\"text\"])\n",
    "            # remove numbers\n",
    "            tweet = re.sub(r'[0-9]', '', tweet)\n",
    "            # remove punctuations\n",
    "            tweet = ' '.join(re.sub(r'[^\\w\\s]', \" \", tweet).split())\n",
    "\n",
    "            comments = \"\"\n",
    "            for row in line[1:]:\n",
    "                # use tweet preprocessor to clean text\n",
    "                comment = p.clean(row[\"text\"])\n",
    "                comment = re.sub(r'[0-9]', '', comment)\n",
    "                # remove punctuations\n",
    "                comment = ' '.join(re.sub(r'[^\\w\\s]', \" \", comment).split())\n",
    "                comments += \" \" + comment + \".\"\n",
    "\n",
    "            data = data.append({\"id\":tweet_id,\"text\":tweet,\"datetime\":datetime,\"comments\":comments,\"hashtags\":hashtags}, ignore_index=True)\n",
    "    f.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_covid_data():\n",
    "    covid_data_ready = False\n",
    "    try:\n",
    "        covid_label = read_label(PROJECT_DATA + 'covid-output.json')\n",
    "        covid_data_ready = True\n",
    "    except:\n",
    "        print('Covid data not yet created.')\n",
    "        covid_prediction()\n",
    "\n",
    "    if not covid_data_ready:\n",
    "        covid_label = read_label(PROJECT_DATA + 'covid-output.json')\n",
    "\n",
    "    covid_data = read_covid_data()\n",
    "    covid_df = pd.merge(covid_data, covid_label, on=\"id\", how=\"outer\")\n",
    "\n",
    "    # Save covid data to csv for later use/re-use without having to re-execute everything\n",
    "    open(PROJECT_DATA + 'covid-data.csv','w', newline='').write(covid_df.to_csv(index=False))\n",
    "\n",
    "\n",
    "def get_covid_data():\n",
    "    try:\n",
    "        # check if covid data file exists\n",
    "        f = open(PROJECT_DATA + 'covid-data.csv', 'r')\n",
    "        f.close()\n",
    "    except:\n",
    "        prepare_covid_data()\n",
    "\n",
    "    covid_df = pd.read_csv(PROJECT_DATA + 'covid-data.csv', dtype={'id':str})\n",
    "    covid_df = covid_df.assign(datetime=pd.to_datetime(covid_df.datetime))\n",
    "\n",
    "    return covid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-zTst8RyX00"
   },
   "source": [
    "## Sentiment Analysis using TextBlob\n",
    "We apply sentiment analysis on the main tweets and comments using TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4vOjNlZrJkj"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def compute_sentiments(covid_df):\n",
    "    sentiments = pd.DataFrame()\n",
    "    for row in range(len(covid_df)):\n",
    "        if isinstance(covid_df['text'][row], str): # check that there is a tweet or that tweet value is not nan\n",
    "            tweet_pol = TextBlob(covid_df['text'][row]).polarity\n",
    "            tweet_sub = TextBlob(covid_df['text'][row]).subjectivity\n",
    "\n",
    "            if isinstance(covid_df['comments'][row], str): # check that there is a comment or that comment value is not nan\n",
    "                reply_pol = TextBlob(covid_df['comments'][row]).polarity\n",
    "                reply_sub = TextBlob(covid_df['comments'][row]).subjectivity\n",
    "\n",
    "                # Do not include tweets null tweets and null comments in the analysis\n",
    "                sentiments = sentiments.append({\"id\":covid_df[\"id\"][row],\n",
    "                                                \"tweet sentiment\":tweet_pol,\n",
    "                                                \"tweet subjectivity\":tweet_sub,\n",
    "                                                \"reply sentiment\":reply_pol,\n",
    "                                                \"reply subjectivity\":reply_sub} , ignore_index=True)\n",
    "        \n",
    "    return sentiments\n",
    "\n",
    "    \n",
    "def save_sentiments():\n",
    "    covid_df = get_covid_data()\n",
    "    covid_df = covid_df.assign(datetime=pd.to_datetime(covid_df.datetime))\n",
    "\n",
    "    sentiments = compute_sentiments(covid_df)\n",
    "    covid_sentiments = pd.merge(covid_df, sentiments, on=\"id\", how=\"outer\")\n",
    "    # Save covid data to csv for later use/re-use without having to re-execute everything\n",
    "    open(PROJECT_DATA + 'covid-sentiments.csv','w', newline='').write(covid_sentiments.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SR5FpHuIIL2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # check if covid data file exists\n",
    "    f = open(PROJECT_DATA + 'covid-sentiments.csv', 'r')\n",
    "    f.close()\n",
    "except:\n",
    "    save_sentiments()\n",
    "\n",
    "covid_sentiments = pd.read_csv(PROJECT_DATA + 'covid-sentiments.csv', dtype={'id':str})\n",
    "covid_sentiments = covid_sentiments.assign(datetime=pd.to_datetime(covid_sentiments.datetime))\n",
    "\n",
    "group = covid_sentiments.groupby(covid_sentiments['label'])\n",
    "for label, grp in group:\n",
    "    if label == 'non-rumour':\n",
    "        non_rumour = grp\n",
    "    else:\n",
    "        rumour = grp\n",
    "\n",
    "        \n",
    "mean = covid_sentiments.groupby(covid_sentiments['label']).mean()\n",
    "median = covid_sentiments.groupby(covid_sentiments['label']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "5IefvhL-Pa-e",
    "outputId": "6ef7cf6a-39e3-4600-cb62-a1dfbfef6282"
   },
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "t4hkBrIdPcnd",
    "outputId": "c633098c-d373-45cb-bae7-63effe512822"
   },
   "outputs": [],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "Et73fXJ0PjxV",
    "outputId": "88a596b8-bbc1-43e9-a278-716d9f4dae1c"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(non_rumour['tweet sentiment'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "            non_rumour['tweet subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Polarity and subjectivity distribution of tweets classified as non-rumours')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(rumour['tweet sentiment'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "            rumour['tweet subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Polarity and subjectivity distribution of tweets classified as rumours')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(non_rumour['reply sentiment'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "            non_rumour['reply subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Polarity and subjectivity distribution of replies of tweets classified as non-rumours')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(rumour['reply sentiment'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "            rumour['reply subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Polarity and subjectivity distribution of replies of tweets classified as rumours')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "xqUk5yc5B-tx",
    "outputId": "41f564d8-b87d-431a-8f4d-3dee70bb3f9d"
   },
   "outputs": [],
   "source": [
    "non_rumour['datetime'] = pd.to_datetime(non_rumour['datetime'], format='%y-%m-%d %H:%M:%S')\n",
    "rumour['datetime'] = pd.to_datetime(rumour['datetime'], format='%y-%m-%d %H:%M:%S')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(non_rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         non_rumour['tweet sentiment'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Polarity distribution of tweets classified as non-rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         rumour['tweet sentiment'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Polarity distribution of tweets classified as rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(non_rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         non_rumour['reply sentiment'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Polarity distribution of replies of tweets classified as non-rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         rumour['reply sentiment'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Polarity distribution of replies of tweets classified as rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "7K1_h-S98Zkh",
    "outputId": "b464286b-4e49-4559-efd2-39f1a00416a3"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(non_rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         non_rumour['tweet subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Subjectivity distribution of tweets classified as non-rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         rumour['tweet subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Subjectivity distribution of tweets classified as rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(non_rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         non_rumour['reply subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "plt.title('Subjectivity distribution of replies of tweets classified as non-rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(rumour['datetime'].sample(1000, random_state=np.random.RandomState(SEED)), \n",
    "         rumour['reply subjectivity'].sample(1000, random_state=np.random.RandomState(SEED)))\n",
    "\n",
    "plt.title('Subjectivity distribution of replies of tweets classified as rumours across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "6dLRhy5uRB5N",
    "outputId": "e848eb4a-3d15-4a1e-d7fc-b5aa4f828b4a"
   },
   "outputs": [],
   "source": [
    "nrT = non_rumour['datetime']\n",
    "rT = rumour['datetime']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "non_rumour['tweet sentiment'].groupby([nrT.dt.year, nrT.dt.month]).mean().plot(kind=\"line\", color='b', label='non-rumour')\n",
    "rumour['tweet sentiment'].groupby([rT.dt.year, rT.dt.month]).mean().plot(kind=\"line\", color='r', label='rumour')\n",
    "plt.title('Mean polarity distribution of tweets across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "non_rumour['tweet subjectivity'].groupby([nrT.dt.year, nrT.dt.month]).mean().plot(kind=\"line\", color='b', label='non-rumour')\n",
    "rumour['tweet subjectivity'].groupby([rT.dt.year, rT.dt.month]).mean().plot(kind=\"line\", color='r', label='rumour')\n",
    "plt.title('Mean subjectivity distribution of tweets across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "non_rumour['reply sentiment'].groupby([nrT.dt.year, nrT.dt.month]).mean().plot(kind=\"line\", color='b', label='non-rumour')\n",
    "rumour['reply sentiment'].groupby([rT.dt.year, rT.dt.month]).mean().plot(kind=\"line\", color='r', label='rumour')\n",
    "plt.title('Mean polarity distribution of replies of tweets across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Polarity')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "non_rumour['reply subjectivity'].groupby([nrT.dt.year, nrT.dt.month]).mean().plot(kind=\"line\", color='b', label='non-rumour')\n",
    "rumour['reply subjectivity'].groupby([rT.dt.year, rT.dt.month]).mean().plot(kind=\"line\", color='r', label='rumour')\n",
    "plt.title('Mean subjectivity distribution of replies of tweets across time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHNR9EP0RbTw"
   },
   "source": [
    "# Popular Hashtags\n",
    "In this section, we use WordCloud to visualise the popular hashtags of rumour and non-rumour tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nG88i-3pRteS",
    "outputId": "d6877858-d442-4bdd-c751-7d78116372f3"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "# segmenter using the word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus=\"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ka2Tf6WuG8o8"
   },
   "outputs": [],
   "source": [
    "for i in range(len(non_rumour)):\n",
    "    if non_rumour.iloc[i]['hashtags'] != [] and isinstance(non_rumour.iloc[i]['hashtags'], str):\n",
    "        hashtag = re.sub(r'[,\\[\\]\\']', '', non_rumour.iloc[i]['hashtags'].lower()).split()\n",
    "        hashtag = ' '.join([str(elem) for elem in hashtag])\n",
    "        non_rumour.loc[i,'segmented'] = seg_tw.segment(hashtag)\n",
    "\n",
    "for i in range(len(rumour)):\n",
    "    if rumour.iloc[i]['hashtags'] != [] and isinstance(rumour.iloc[i]['hashtags'], str):\n",
    "        hashtag = re.sub(r'[,\\[\\]\\']', '', rumour.iloc[i]['hashtags'].lower()).split()\n",
    "        hashtag = ' '.join([str(elem) for elem in hashtag])\n",
    "        rumour.loc[i,'segmented'] = seg_tw.segment(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "gyi93VhPOU-t",
    "outputId": "0cd16153-ba73-49c4-a74b-03e780948101"
   },
   "outputs": [],
   "source": [
    "#WordCloud\n",
    "wc = WordCloud(width=800, height=400, max_words=20)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "#Frequency of words\n",
    "nr_fd = FreqDist(non_rumour['segmented'])\n",
    "nr_wc = wc.generate_from_frequencies(nr_fd)\n",
    "plt.imshow(nr_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Popular hashtags of Covid-19 non-rumour tweets\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "#Frequency of words\n",
    "r_fd = FreqDist(rumour['segmented'])\n",
    "r_wc = wc.generate_from_frequencies(r_fd)\n",
    "plt.imshow(r_wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Popular hashtags of Covid-19 rumour tweets\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Zb1fmOetvr9",
    "nFSFaXqxyhaH"
   ],
   "name": "project_classifier_vs_ver.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
